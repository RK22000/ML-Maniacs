{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "events_csv = 'data/train_events.csv'\n",
    "data_csv = \"data/train_series.parquet\"\n",
    "\n",
    "def read_data(verbose=True, rd=True, re=True):\n",
    "    events, data = None, None\n",
    "    if re:\n",
    "        if verbose: \n",
    "            print(f\"Reading {events_csv}\")\n",
    "            t=time.time()\n",
    "        # events = pd.read_csv(events_csv)\n",
    "        events = dd.read_csv(events_csv)\n",
    "        if verbose: \n",
    "            print(f\"Read {len(events)} rows from {events_csv} in {time.time()-t:.3f} seconds\")\n",
    "    if rd:\n",
    "        if verbose:\n",
    "            print(f\"Reading {data_csv}\")\n",
    "            t=time.time()\n",
    "        data = dd.read_parquet(data_csv)# .read_parquet(data_csv, engine='fastparquet')\n",
    "        if verbose: \n",
    "            print(f\"Read {len(data)} rows from {data_csv} in {time.time()-t:.3f} seconds\")\n",
    "    return data, events\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/train_events.csv\n",
      "Read 14508 rows from data/train_events.csv in 0.029 seconds\n",
      "Reading data/train_series.parquet\n",
      "Read 127946340 rows from data/train_series.parquet in 26.384 seconds\n"
     ]
    }
   ],
   "source": [
    "data,events = read_data()\n",
    "# train_sids, test_sids = train_test_split(events['series_id'].unique(), train_size=0.8, random_state=42)\n",
    "# train_sids.shape, test_sids.shape\n",
    "# train_sids, test_sids = [set(i) for i in [train_sids, test_sids]]\n",
    "# len(train_sids), len(test_sids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowMemoryError",
     "evalue": "malloc of size 1822162944 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\ChildMindSleep\\daskDF.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/ChildMindSleep/daskDF.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:1437\u001b[0m, in \u001b[0;36m_Frame.head\u001b[1;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[39m# No need to warn if we're already looking at all partitions\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m safe \u001b[39m=\u001b[39m npartitions \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnpartitions\n\u001b[1;32m-> 1437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_head(n\u001b[39m=\u001b[39mn, npartitions\u001b[39m=\u001b[39mnpartitions, compute\u001b[39m=\u001b[39mcompute, safe\u001b[39m=\u001b[39msafe)\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\core.py:1471\u001b[0m, in \u001b[0;36m_Frame._head\u001b[1;34m(self, n, npartitions, compute, safe)\u001b[0m\n\u001b[0;32m   1466\u001b[0m result \u001b[39m=\u001b[39m new_dd_object(\n\u001b[0;32m   1467\u001b[0m     graph, name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta, [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[npartitions]]\n\u001b[0;32m   1468\u001b[0m )\n\u001b[0;32m   1470\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[1;32m-> 1471\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mcompute()\n\u001b[0;32m   1472\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39m, traverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[39mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    630\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:96\u001b[0m, in \u001b[0;36mParquetFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(part, \u001b[39mlist\u001b[39m):\n\u001b[0;32m     94\u001b[0m     part \u001b[39m=\u001b[39m [part]\n\u001b[1;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m read_parquet_part(\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs,\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine,\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta,\n\u001b[0;32m    100\u001b[0m     [\n\u001b[0;32m    101\u001b[0m         \u001b[39m# Temporary workaround for HLG serialization bug\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[39m# (see: https://github.com/dask/dask/issues/8581)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m         (p\u001b[39m.\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mpiece\u001b[39m\u001b[39m\"\u001b[39m], p\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m\"\u001b[39m, {}))\n\u001b[0;32m    104\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(p, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    105\u001b[0m         \u001b[39melse\u001b[39;00m (p[\u001b[39m\"\u001b[39m\u001b[39mpiece\u001b[39m\u001b[39m\"\u001b[39m], p\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m\"\u001b[39m, {}))\n\u001b[0;32m    106\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m part\n\u001b[0;32m    107\u001b[0m     ],\n\u001b[0;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns,\n\u001b[0;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex,\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_kwargs,\n\u001b[0;32m    111\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:659\u001b[0m, in \u001b[0;36mread_parquet_part\u001b[1;34m(fs, engine, meta, part, columns, index, kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(part) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m part[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m check_multi_support(engine):\n\u001b[0;32m    657\u001b[0m     \u001b[39m# Part kwargs expected\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     func \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mread_partition\n\u001b[1;32m--> 659\u001b[0m     dfs \u001b[39m=\u001b[39m [\n\u001b[0;32m    660\u001b[0m         func(\n\u001b[0;32m    661\u001b[0m             fs,\n\u001b[0;32m    662\u001b[0m             rg,\n\u001b[0;32m    663\u001b[0m             columns\u001b[39m.\u001b[39mcopy(),\n\u001b[0;32m    664\u001b[0m             index,\n\u001b[0;32m    665\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtoolz\u001b[39m.\u001b[39mmerge(kwargs, kw),\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m         \u001b[39mfor\u001b[39;00m (rg, kw) \u001b[39min\u001b[39;00m part\n\u001b[0;32m    668\u001b[0m     ]\n\u001b[0;32m    669\u001b[0m     df \u001b[39m=\u001b[39m concat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m dfs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    670\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[39m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39m# list of parts at once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:660\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(part) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m part[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m check_multi_support(engine):\n\u001b[0;32m    657\u001b[0m     \u001b[39m# Part kwargs expected\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     func \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mread_partition\n\u001b[0;32m    659\u001b[0m     dfs \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 660\u001b[0m         func(\n\u001b[0;32m    661\u001b[0m             fs,\n\u001b[0;32m    662\u001b[0m             rg,\n\u001b[0;32m    663\u001b[0m             columns\u001b[39m.\u001b[39mcopy(),\n\u001b[0;32m    664\u001b[0m             index,\n\u001b[0;32m    665\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtoolz\u001b[39m.\u001b[39mmerge(kwargs, kw),\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m         \u001b[39mfor\u001b[39;00m (rg, kw) \u001b[39min\u001b[39;00m part\n\u001b[0;32m    668\u001b[0m     ]\n\u001b[0;32m    669\u001b[0m     df \u001b[39m=\u001b[39m concat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m dfs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    670\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    671\u001b[0m     \u001b[39m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39m# list of parts at once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:627\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_partition\u001b[1;34m(cls, fs, pieces, columns, index, dtype_backend, categories, partitions, filters, schema, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m     row_group \u001b[39m=\u001b[39m [row_group]\n\u001b[0;32m    626\u001b[0m \u001b[39m# Read in arrow table and convert to pandas\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m arrow_table \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_read_table(\n\u001b[0;32m    628\u001b[0m     path_or_frag,\n\u001b[0;32m    629\u001b[0m     fs,\n\u001b[0;32m    630\u001b[0m     row_group,\n\u001b[0;32m    631\u001b[0m     columns,\n\u001b[0;32m    632\u001b[0m     schema,\n\u001b[0;32m    633\u001b[0m     filters,\n\u001b[0;32m    634\u001b[0m     partitions,\n\u001b[0;32m    635\u001b[0m     partition_keys,\n\u001b[0;32m    636\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    637\u001b[0m )\n\u001b[0;32m    638\u001b[0m \u001b[39mif\u001b[39;00m multi_read:\n\u001b[0;32m    639\u001b[0m     tables\u001b[39m.\u001b[39mappend(arrow_table)\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:1760\u001b[0m, in \u001b[0;36mArrowDatasetEngine._read_table\u001b[1;34m(cls, path_or_frag, fs, row_groups, columns, schema, filters, partitions, partition_keys, **kwargs)\u001b[0m\n\u001b[0;32m   1753\u001b[0m     arrow_table \u001b[39m=\u001b[39m frag\u001b[39m.\u001b[39mto_table(\n\u001b[0;32m   1754\u001b[0m         use_threads\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1755\u001b[0m         schema\u001b[39m=\u001b[39mschema,\n\u001b[0;32m   1756\u001b[0m         columns\u001b[39m=\u001b[39mcols,\n\u001b[0;32m   1757\u001b[0m         \u001b[39mfilter\u001b[39m\u001b[39m=\u001b[39m_filters_to_expression(filters) \u001b[39mif\u001b[39;00m filters \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1758\u001b[0m     )\n\u001b[0;32m   1759\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1760\u001b[0m     arrow_table \u001b[39m=\u001b[39m _read_table_from_path(\n\u001b[0;32m   1761\u001b[0m         path_or_frag,\n\u001b[0;32m   1762\u001b[0m         fs,\n\u001b[0;32m   1763\u001b[0m         row_groups,\n\u001b[0;32m   1764\u001b[0m         columns,\n\u001b[0;32m   1765\u001b[0m         schema,\n\u001b[0;32m   1766\u001b[0m         filters,\n\u001b[0;32m   1767\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1768\u001b[0m     )\n\u001b[0;32m   1770\u001b[0m \u001b[39m# For pyarrow.dataset api, if we did not read directly from\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[39m# fragments, we need to add the partitioned columns here.\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[39mif\u001b[39;00m partitions \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(partitions, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\arrow.py:254\u001b[0m, in \u001b[0;36m_read_table_from_path\u001b[1;34m(path, fs, row_groups, columns, schema, filters, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mwith\u001b[39;00m _open_input_files(\n\u001b[0;32m    248\u001b[0m     [path],\n\u001b[0;32m    249\u001b[0m     fs\u001b[39m=\u001b[39mfs,\n\u001b[0;32m    250\u001b[0m     precache_options\u001b[39m=\u001b[39mprecache_options,\n\u001b[0;32m    251\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopen_file_options,\n\u001b[0;32m    252\u001b[0m )[\u001b[39m0\u001b[39m] \u001b[39mas\u001b[39;00m fil:\n\u001b[0;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m row_groups \u001b[39m==\u001b[39m [\u001b[39mNone\u001b[39;00m]:\n\u001b[1;32m--> 254\u001b[0m         \u001b[39mreturn\u001b[39;00m pq\u001b[39m.\u001b[39mParquetFile(fil, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpre_buffer)\u001b[39m.\u001b[39mread(\n\u001b[0;32m    255\u001b[0m             columns\u001b[39m=\u001b[39mcolumns,\n\u001b[0;32m    256\u001b[0m             use_threads\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    257\u001b[0m             use_pandas_metadata\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mread_kwargs,\n\u001b[0;32m    259\u001b[0m         )\n\u001b[0;32m    260\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m         \u001b[39mreturn\u001b[39;00m pq\u001b[39m.\u001b[39mParquetFile(fil, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpre_buffer)\u001b[39m.\u001b[39mread_row_groups(\n\u001b[0;32m    262\u001b[0m             row_groups,\n\u001b[0;32m    263\u001b[0m             columns\u001b[39m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mread_kwargs,\n\u001b[0;32m    267\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:628\u001b[0m, in \u001b[0;36mParquetFile.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[39mRead a Table from Parquet format.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39manimal: [[\"Flamingo\",\"Parrot\",...,\"Brittle stars\",\"Centipede\"]]\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    626\u001b[0m column_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_column_indices(\n\u001b[0;32m    627\u001b[0m     columns, use_pandas_metadata\u001b[39m=\u001b[39muse_pandas_metadata)\n\u001b[1;32m--> 628\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreader\u001b[39m.\u001b[39mread_all(column_indices\u001b[39m=\u001b[39mcolumn_indices,\n\u001b[0;32m    629\u001b[0m                             use_threads\u001b[39m=\u001b[39muse_threads)\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\pyarrow\\_parquet.pyx:1375\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetReader.read_all\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rahul\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: malloc of size 1822162944 failed"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14508"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df:dd.DataFrame = dd.read_csv(events_csv)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127946340"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.head(1279463)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
