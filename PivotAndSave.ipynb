{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pivot and Save train_series.parquet\n",
    "\n",
    "This Notebook will walk you through how we preprocessed the really long train_series.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\\train_series.parquet\n",
      "Read 127946340 rows from data\\train_series.parquet in 15.336 seconds\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import os\n",
    "import time\n",
    "data_parquet = os.path.join('data','train_series.parquet')\n",
    "print(f\"Reading {data_parquet}\")\n",
    "t=time.time()\n",
    "# data: dd.DataFrame = dd.read_parquet(data_parquet, columns=['step', 'series_id', 'anglez', 'enmo'], categories=['series_id'])\n",
    "data: dd.DataFrame = dd.read_parquet(data_parquet)#, columns=['step', 'series_id', 'anglez', 'enmo'], categories=['series_id'])\n",
    "print(f\"Read {len(data)} rows from {data_parquet} in {time.time()-t:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>step</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>anglez</th>\n",
       "      <th>enmo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>uint32</td>\n",
       "      <td>object</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              series_id    step timestamp   anglez     enmo\n",
       "npartitions=1                                              \n",
       "                 object  uint32    object  float32  float32\n",
       "                    ...     ...       ...      ...      ...\n",
       "Dask Name: read-parquet, 1 graph layer"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to save individual series files\n",
    "I'm calling this attempt failed cuz its taking too long to save all the series files.\n",
    "I will try to set up a process to process all the files over night though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = data['series_id'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join('data', 'parsed')\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdone = os.listdir(dir_path)\n",
    "sdone = set([i[:-8] for i in sdone if os.path.exists(os.path.join(dir_path, i, 'done.txt'))])\n",
    "sids = [i for i in sids if i not in sdone]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['785c9ca4eff7',\n",
       "  'cfeb11428dd7',\n",
       "  '9ddd40f2cb36',\n",
       "  'b84960841a75',\n",
       "  '6a4cd123bd69',\n",
       "  'ca732a3c37f7',\n",
       "  '51fdcc8d9fe7',\n",
       "  '939932f1822d',\n",
       "  'ea0770830757',\n",
       "  '390b487231ce'],\n",
       " 'total length = 250')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sids[:10], f'total length = {len(sids)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = data.groupby('series_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 6 cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [05:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import parr\n",
    "from itertools import repeat\n",
    "importlib.reload(parr)\n",
    "dfs = [grouped_df.get_group(i) for i in sids]\n",
    "parr.parallelize(parr.save_series_id_to_parquet, dfs, sids, repeat(dir_path), total=len(sids))\n",
    "# parr.parallelize(parr.hello2, repeat('hello',10), total=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▏                             | 27/277 [7:12:35<66:45:33, 961.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sid \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(sids):\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m grouped_df\u001b[38;5;241m.\u001b[39mget_group(sid)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(sid, file\u001b[38;5;241m=\u001b[39mf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:1062\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m out \u001b[38;5;241m=\u001b[39m Scalar(graph, final_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m-> 1062\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m fs\u001b[38;5;241m.\u001b[39minvalidate_cache(path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:342\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    319\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:628\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 628\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "with(open('log.txt', 'w')) as f:\n",
    "    f.write('')\n",
    "for sid in tqdm.tqdm(sids):\n",
    "    df = grouped_df.get_group(sid)\n",
    "    dd.to_parquet(df, os.path.join(dir_path, f'{sid}.parquet'), write_index=False)\n",
    "    with(open('log.txt', 'a')) as f:\n",
    "        print(sid, file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('log.txt', 'r')) as f:\n",
    "    lines = f.readlines()\n",
    "    lines = set([i[:-1] for i in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sids = set([i for i in sids if i not in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'snappy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwrite_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mappend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mignore_divisions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpartition_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcustom_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwrite_metadata_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompute\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcompute_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'infer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mname_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfilesystem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Store Dask.dataframe to Parquet files\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Each partition will be written to a separate file.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "df : dask.dataframe.DataFrame\n",
       "path : string or pathlib.Path\n",
       "    Destination directory for data.  Prepend with protocol like ``s3://``\n",
       "    or ``hdfs://`` for remote data.\n",
       "engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n",
       "    Parquet library to use. Defaults to 'auto', which uses ``pyarrow`` if\n",
       "    it is installed, and falls back to ``fastparquet`` otherwise.\n",
       "compression : string or dict, default 'snappy'\n",
       "    Either a string like ``\"snappy\"`` or a dictionary mapping column names\n",
       "    to compressors like ``{\"name\": \"gzip\", \"values\": \"snappy\"}``. Defaults\n",
       "    to ``\"snappy\"``.\n",
       "write_index : boolean, default True\n",
       "    Whether or not to write the index. Defaults to True.\n",
       "append : bool, default False\n",
       "    If False (default), construct data-set from scratch. If True, add new\n",
       "    row-group(s) to an existing data-set. In the latter case, the data-set\n",
       "    must exist, and the schema must match the input data.\n",
       "overwrite : bool, default False\n",
       "    Whether or not to remove the contents of `path` before writing the dataset.\n",
       "    The default is False.  If True, the specified path must correspond to\n",
       "    a directory (but not the current working directory).  This option cannot\n",
       "    be set to True if `append=True`.\n",
       "    NOTE: `overwrite=True` will remove the original data even if the current\n",
       "    write operation fails.  Use at your own risk.\n",
       "ignore_divisions : bool, default False\n",
       "    If False (default) raises error when previous divisions overlap with\n",
       "    the new appended divisions. Ignored if append=False.\n",
       "partition_on : list, default None\n",
       "    Construct directory-based partitioning by splitting on these fields'\n",
       "    values. Each dask partition will result in one or more datafiles,\n",
       "    there will be no global groupby.\n",
       "storage_options : dict, default None\n",
       "    Key/value pairs to be passed on to the file-system backend, if any.\n",
       "custom_metadata : dict, default None\n",
       "    Custom key/value metadata to include in all footer metadata (and\n",
       "    in the global \"_metadata\" file, if applicable).  Note that the custom\n",
       "    metadata may not contain the reserved b\"pandas\" key.\n",
       "write_metadata_file : bool or None, default None\n",
       "    Whether to write the special ``_metadata`` file. If ``None`` (the\n",
       "    default), a ``_metadata`` file will only be written if ``append=True``\n",
       "    and the dataset already has a ``_metadata`` file.\n",
       "compute : bool, default True\n",
       "    If ``True`` (default) then the result is computed immediately. If\n",
       "    ``False`` then a ``dask.dataframe.Scalar`` object is returned for\n",
       "    future computation.\n",
       "compute_kwargs : dict, default True\n",
       "    Options to be passed in to the compute method\n",
       "schema : pyarrow.Schema, dict, \"infer\", or None, default \"infer\"\n",
       "    Global schema to use for the output dataset. Defaults to \"infer\", which\n",
       "    will infer the schema from the dask dataframe metadata. This is usually\n",
       "    sufficient for common schemas, but notably will fail for ``object``\n",
       "    dtype columns that contain things other than strings. These columns\n",
       "    will require an explicit schema be specified. The schema for a subset\n",
       "    of columns can be overridden by passing in a dict of column names to\n",
       "    pyarrow types (for example ``schema={\"field\": pa.string()}``); columns\n",
       "    not present in this dict will still be automatically inferred.\n",
       "    Alternatively, a full ``pyarrow.Schema`` may be passed, in which case\n",
       "    no schema inference will be done. Passing in ``schema=None`` will\n",
       "    disable the use of a global file schema - each written file may use a\n",
       "    different schema dependent on the dtypes of the corresponding\n",
       "    partition. Note that this argument is ignored by the \"fastparquet\"\n",
       "    engine.\n",
       "name_function : callable, default None\n",
       "    Function to generate the filename for each output partition.\n",
       "    The function should accept an integer (partition index) as input and\n",
       "    return a string which will be used as the filename for the corresponding\n",
       "    partition. Should preserve the lexicographic order of partitions.\n",
       "    If not specified, files will created using the convention\n",
       "    ``part.0.parquet``, ``part.1.parquet``, ``part.2.parquet``, ...\n",
       "    and so on for each partition in the DataFrame.\n",
       "filesystem: \"fsspec\", \"arrow\", or fsspec.AbstractFileSystem backend to use.\n",
       "    Note that the \"fastparquet\" engine only supports \"fsspec\" or an explicit\n",
       "    ``pyarrow.fs.AbstractFileSystem`` object. Default is \"fsspec\".\n",
       "**kwargs :\n",
       "    Extra options to be passed on to the specific backend.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> df = dd.read_csv(...)  # doctest: +SKIP\n",
       ">>> df.to_parquet('/path/to/output/', ...)  # doctest: +SKIP\n",
       "\n",
       "By default, files will be created in the specified output directory using the\n",
       "convention ``part.0.parquet``, ``part.1.parquet``, ``part.2.parquet``, ... and so on for\n",
       "each partition in the DataFrame. To customize the names of each file, you can use the\n",
       "``name_function=`` keyword argument. The function passed to ``name_function`` will be\n",
       "used to generate the filename for each partition and should expect a partition's index\n",
       "integer as input and return a string which will be used as the filename for the corresponding\n",
       "partition. Strings produced by ``name_function`` must preserve the order of their respective\n",
       "partition indices.\n",
       "\n",
       "For example:\n",
       "\n",
       ">>> name_function = lambda x: f\"data-{x}.parquet\"\n",
       ">>> df.to_parquet('/path/to/output/', name_function=name_function)  # doctest: +SKIP\n",
       "\n",
       "will result in the following files being created::\n",
       "\n",
       "    /path/to/output/\n",
       "        ├── data-0.parquet\n",
       "        ├── data-1.parquet\n",
       "        ├── data-2.parquet\n",
       "        └── ...\n",
       "\n",
       "See Also\n",
       "--------\n",
       "read_parquet: Read parquet data to dask.dataframe\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\rahul\\anaconda3\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd.to_parquet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Attempt to pivot and save\n",
    "kinda failed cus the process needed too much memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['series_id'] = data['series_id'].cat.as_known()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dd.reshape.pivot_table(data, index='step', columns='series_id', values=['anglez','enmo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglez = pivoted['anglez']\n",
    "anglez.columns = anglez.columns.astype('string')\n",
    "anglez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglez.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anglez.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enmo = pivoted['enmo']\n",
    "enmo.columns = enmo.columns.astype('string')\n",
    "enmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join('data', 'parsed')\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_parquet(enmo, os.path.join(*dir_path, 'enmo.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_parquet(anglez, os.path.join(*dir_path, 'anglez.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
